[
	{
		"paper_text": "7 \n\nImplementing Text Classification with Feed Forward Networks \n\nIn this chapter we provide an implementation of the multilayer neural network described in Chapter 5, along with several of the best practices discussed in Chapter 6. \n\nRemaining fairly simple, our network will consist of three neuron layers that are fully connected: an input layer that stores the input features, a hidden intermediate layer, and an output layer that produces the scores for each class to be learned.\nIn between these layers we will include dropout and a nonlinearity (ReLU). \n\nSidebar 7.1 The PyTorch Linear layer implements the connections between layers of neurons. \n\nBefore discussing the implementation of more complex neural archi- tectures in PyTorch, it is important to address one potential source of confusion.\nIn PyTorch, the Linear layer implements the connections be- tween two layers of neurons rather than an actual neuron layer.\nThat is, a Linear object contains the weights Wl+1 that connect the neu- rons in layer l with the neurons in layer l + 1 in Figure 5.2.\nThis is why the Linear constructor includes two dimensions: one for the in- put neuron layer (in_features) and one for the output neuron layer (out_features).\nOptionally, if the parameter bias is set to True, the corresponding Linear object also contains the bias weights for the out- put neurons, i.e., bl+1 in Figure 5.2.\nThus, in our Model with three neuron layers, we will have two Linear objects. \n\nTo stay close to the code, from this point forward when we mention the term layer in the implementation chapters, we refer to a PyTorch Linear layer, unless stated otherwise. \n\n109 \n\n  \n\n110 Implementing Text Classification with Feed Forward Networks \n\nFurther, we make use of two PyTorch classes: a Dataset and a DataLoader.\nThe advantage of using these classes is that they make several things easy, including data shuffling and batching.\nLastly, since the classifier\u2019s architecture has become more complex, for optimization we transition from stochastic gradient descent to the Adam optimizer to take advan- tage of its additional features such as momentum, and L2 regularization. \n\nAs before, the code from this chapter is available in a Jupyter notebook: chap7_ffnn. \n\n7.1 Data \n\nIn this chapter we continue to use the AG News Dataset (Section 4.2.1), including the same loading and preprocessing steps.\nAlso, we continue using the same train and test sets to be able to compare results to the ones obtained in Section 4.2.\nHowever, in this chapter we will make use of a development set to tune the model\u2019s hyper parameters.\nFor this purpose, we split the training set in two: 80% of the examples become a new training set, while the other 20% are the development set: \n\nIn the code above we used scikit-learn\u2019s train_test_split function to split the training set into a development partition and a new training partition.\nNote that this function can split Python lists, NumPy arrays, and even Pandas dataframes.\nThe returned dataframes preserve the in- dex of the original training dataframe, which can be useful to keep the connection to the original data, but is not what we currently need, as we are trying to create two independent datasets.\nTherefore, we reset the index of the two new dataframes. \n\nA second difference to what was done in Section 4.2 is the introduction of mini-batches.\nPyTorch provides the DataLoader1 class which can be used for shuffling the data and splitting it into mini-batches.\nIn order to create a DataLoader, we need the data to be in the form of a PyTorch Dataset.2\nThere are two main types of PyTorch datasets: map-style and iterable-style.\nWe will use the former, as it is simpler and meets our needs, but it is good to know that the other option is available for situations when, for example, you need to stream data from a remote source or random access is expensive. \n\nTo create a map-style dataset we need to subclass torch.utils.data.\nDataset and override its __getitem__() method (to return an example given a \n\n1 https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader 2 https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset \n\n  \n\n7.2 Fully-Connected Neural Network 111 \n\nkey), as well as its __len__() method (to return the number of exam- ples in the dataset).\nOur dataset implementation stores two sequences: one for holding the features, and another for storing the corresponding labels.\nIn our implementation we store two Pandas Series, but Python lists or NumPy arrays would also work.\nThe implementation __len__() is trivial: we simply return the length of the feature sequence, that is, the number of feature vectors.\nThe implementation of __getitem__() is slightly more involved.\nRecall that each of our feature vectors is rep- resented as a dictionary with word ids as keys, and word counts as values, and any word id not in the dictionary has a count of zero.\nOur __getitem__() method transforms this representation into one that Py- Torch can use.\nWe first create two PyTorch tensors, one for the label and one for the features, which is initially populated with zeros.\nThen, we retrieve the feature dictionary corresponding to the provided index, and, for each key-value pair in the feature dictionary, we update the corre- sponding element of the tensor.\nOnce this is complete, we return the feature and label tensors for the datum: \n\n7.2 Fully-Connected Neural Network \n\nHaving completed the Dataset implementation, we next implement the model, i.e., a fully-connected neural network with two layers.3\nIn Sec- tion 4.2 we used a Linear module directly to implement the simpler models discussed there.\nThis time, we will demonstrate how to imple- ment a model as a new module, by subclassing torch.nn.\nModule.\nAl- though this is not necessary for this model, as it can be represented by a Sequential module, as models get more complex, it becomes help- ful to encapsulate their behavior.\nTo implement a Module, we need to implement the constructor and override the forward() method. \n\nNote that, in our constructor below, before initializing the object fields, we invoke the constructor of the parent class (i.e., Module) with the line super().__init__().\nThis allows PyTorch to set up the mech- anisms through which any layers defined as attributes in the construc- tor are properly registered as model parameters.\nIn our example, a Sequential instance is assigned to self.layers; this is enough for our model instance to know about it during back-propagation and parameter updating. \n\n3 Recall that layer here refers to the PyTorch Linear layer that contains the connections between two neuron layers.\nSee Sidebar 7.1 for more details. \n\n112 Implementing Text Classification with Feed Forward Networks \n\nHere, our model consists of two linear layers, each one preceded by a dropout layer (which drops out input neurons from the corresponding linear layer).\nThe input of the first linear layer has the same size as our vocabulary, and its output has the dimension of the hidden neuron layer (please see Section 5.1 for a refresher on the architecture of the feed-forward neural network).\nConsequently, the input size of the second linear layer is equal to the size of the hidden layer, and its output size is the number of classes.\nAdditionally, between the two linear layers we add a ReLU nonlinearity.4 All of the model layers are wrapped in a Sequential module, which simply connects the output of one layer to the input of the next. \n\nThe second method we need to implement is the forward() method, which defines how the model applies its layers to a given input during the forward pass.\nOur forward() method simply calls the sequential layer and returns its output.\nNote that while this method implements the model\u2019s forward pass, in general, this method should not be called directly by the user.\nInstead, the user should use the model as though it were a function (technically, invoking the __call__() method), and let PyTorch call the forward() method internally.\nThis allows PyTorch to activate necessary features such as module hooks correctly. \n\n7.3 Training \n\nIn order to train our model, we will first initialize the hyperparameters and the different components we need: model, loss function, optimizer, dataset, and data-loader.\nNotable differences with respect to Section 4.2 are the use of the Adam optimizer with a weight decay (this is just what PyTorch calls L2 regularization \u2013 see Chapter 6), and the use of a data- loader with shuffling and batches of 500 examples.\nWe encourage you to take the time to examine the values we use for the hyper parameters, and to experiment with modifying them in the Jupyter notebook. \n\nThe basic steps of the learning loop are the same as those in Sec- \n\n4 Note that nonlinearities such as the ReLU function here are necessary to guarantee that the neural network can learn non-linear decision boundaries.\nSee Chapter 5 for an extended discussion on this topic.\nFurther, nonlinearities can be added after each network layer, but, typically, the output layer is omitted.\nThis is because a softmax or sigmoid function usually follows it.\nIn PyTorch, the nn.CrossEntropyFunction, which we also use in this chapter, includes such a softmax function. \n\n7.3 Training 113 tion 4.2, except that we are now using a development set to keep track \n\nof the performance of the current model after each training epoch. \n\nOne important difference between using our model during training and evaluation is that, prior to each training session, we need to set the model to training mode using the train() method, and before evalu- ating on the development set, we need to set the model to evaluation mode using the eval() method.\nThis is important, because some layers have different behavior depending on whether the model is in training or evaluation mode.\nIn our model, this is the case for the Dropout layer, which randomly zeroes some of its input elements during training and scales its outputs accordingly (see Section 6.6), but during evaluation does nothing. \n\nIn order to plot some relevant statistics acquired from the training data, we collect the current loss and accuracy for each mini-batch.\nNote that we call detach() on the tensors corresponding to the loss and the predicted/gold labels so they are no longer considered when computing gradients.\nCalling cpu() copies the tensors from the GPU to the CPU if we are using the GPU; otherwise it does nothing.\nCalling numpy() converts the PyTorch tensor into a NumPy array.\nUnlike the prediction sequence, which is represented as a vector of label scores, the loss is a scalar.\nFor this reason, we retrieve it as a Python number using the item() method. \n\nWhen evaluating on the development set, since we do not need to compute the gradients, we save computation by wrapping the steps in a torch.no_grad() context-manager.\nSince we are not learning, we do not perform back-propagation or invoke the optimizer. \n\nAfter completing training we have gathered the loss and accuracy val- ues after each epoch for both the training and development partitions.\nNext, we plot these values in order to visualize the classifier\u2019s progress over time.\nPlots such as these are important to determine how well our model is learning, which informs decisions regarding adjusting hyper pa- rameters or modifying the model\u2019s architecture.\nBelow we only show the plot for the loss.\nPlotting the accuracy is very similar; the corresponding code as well as the plot itself is available in the Jupyter notebook. \n\n114 Implementing Text Classification with Feed Forward Networks \n\n                       \n\nThe plot indicates that both the training and development losses de- crease over time.\nThis is good!\nIt indicates that our classifier is neither overfitting nor underfitting.\nRecall from Chapter 2 that overfitting hap- pens when a classifier performs well in training, but poorly on unseen data.\nIn the plot above this would be indicated by a training loss that continues to decrease, but is associated with a development loss that does not.\nUnderfitting happens when a classifier is unable to learn meaningful associations between the input features and the output labels.\nIn this plot this would be shown as loss curves that do not decrease over time. \n\nThis analysis means we are ready to evaluate our trained model on the test set, which must be a truly unseen dataset that was not used for training or to tune hyper parameters.\nIn other words, this experiment will indicate how well our model performs \u201cin the wild.\u201d\nBecause we would like these results to be as close as possible to real-world results, the test set should be used sparingly, only after the entire architecture, its trained parameters, and its hyper parameters have been frozen. \n\nWith our feed-forward neural architecture we have achieved an accu- racy of 92%, which is a substantial improvement over the 88% accuracy we obtained in Section 4.2.\nWe strongly suggest that you experiment not only with the different hyper parameters, but also with different model architectures in the Jupyter notebook.\nSuch exercises will help you de- \n\n7.4 Summary 115 velop an intuition about the different effects each design choice has, as \n\nwell as how these decisions interact with each other. \n\n7.4 Summary \n\nIn this chapter we have shown how to implement a feed-forward neural network in PyTorch.\nWe have also introduced several PyTorch features that encourage and simplify deep learning best practices.\nIn particu- lar, the built-in Dataset and DataLoader classes make mini-batching straightforward while still allowing for customization such as sampling.\nThe ability to create a custom Dataset object allows us to handle com- plex data and still have access to the features of a DataLoader.\nBy convention, all the components provided by PyTorch are batch-aware and assume that the first dimension refers to the batch size, simplifying model implementation and improving readability. \n\nIn building the model itself, we also saw that PyTorch uses layer mod- ularization, i.e., both the network layers themselves and operations on them (such as dropout and activation functions) are modeled as layers in a pipeline.\nThis makes it easy to interweave network layers, add various operations between them, and swap activation functions as desired.\nThe weight initialization is also handled automatically when the layers are created, but can be customized as needed. \n\nFurther, one can tailor the training process in PyTorch by adding mo- mentum, adaptive learning rates, and regularization through optimizer selection and configuration.\nIn this chapter, we used the Adam opti- mizer, which, in the authors\u2019 experience, is a good default choice, but there are many other optimizers to choose from.\nWe recommend that the reader read the PyTorch documentation on optimizers for more details: https://pytorch.org/docs/stable/optim.html. \n\n \n",
		"repo_url": "https://github.com/clulab/gentlenlp/tree/e3e5a69ebff7e9bda7476d57631f708c0769527a"
	},
	{
		"paper_text": "4 \n\nImplementing Text Classification Using Perceptron and Logistic Regression \n\nIn the previous chapters we have discussed the theory behind the percep- tron and logistic regression, including mathematical explanations of how and why they are able to learn from examples.\nIn this chapter we will transition from math to code.\nSpecifically, we will discuss how to imple- ment these models in the Python programming language.\nAll the code that we will introduce throughout this book is available online as well: http://clulab.github.io/gentlenlp/. The reader who is not famil- iar with the Python programming language is encouraged to read first Appendix A, for a brief introduction to the language, and Appendix B, for a discussion on how computers encode and preprocess text.\nOnce done, please return here. \n\nTo get a better understanding of how these algorithms work under the hood, we will start by implementing them from scratch.\nHowever, as the book progresses, we will introduce some of the popular tools and libraries that make Python the language of choice for machine learning, e.g., PyTorch,1 and Hugging Face\u2019s transformers.2 \n\nThe code for all the examples in the book is provided in the form of Jupyter notebooks.3\nImportant fragments of these notebooks will be pre- sented in the implementation chapters so that the reader has the whole picture just by reading the book.\nHowever, we strongly encourage you to download the notebooks and execute them yourself.\nWe also encourage you to modify them to conduct your own experiments! \n\n \n\n1 https://pytorch.org\u20282 https://huggingface.co 3 https://jupyter.org/ \n\n55 \n\n56 Implementing Text Classification Using Perceptron and LR 4.1 Binary Classification \n\nWe begin this chapter with binary classification.\nThat is, we aim to train classifiers that assign one of two labels to a given text.\nAs the example for this task, we will train a review classifier using the the Large Movie Review Dataset (Maas et al., 2011).4 We tackle this task by implementing first a binary perceptron classifier, followed by a binary logistic regression one.\nWe will implement the latter both from scratch as well as using PyTorch, so the reader has a clearer understanding on how PyTorch works \u201cunder the hood.\u201d \n\n4.1.1 Large Movie Review Dataset \n\nThis dataset contains movie reviews and their associated scores (between 1 and 10) as provided by IMDb.5 converted these scores to binary labels by assigning each review a positive or negative label if the review score was above 6 or below 5, respectively.\nReviews with scores 5 and 6 were considered too neutral and thus excluded.\nWe follow the same protocol in this chapter. \n\nThe dataset is divided in two even partitions called train and test, each containing 25,000 reviews.\nThe dataset also provides additional unlabeled reviews, but we will not use those here.\nEach partition con- tains two directories called pos and neg where the positive and negative examples are stored.\nEach review is stored in an independent text file, whose name is composed of an id unique to the partition and the score associated with the review, separated by an underscore.\nAn example of a positive and a negative review is shown in Table 4.1. \n\n4.1.2 Bag-of-words Model \n\nAs discussed in Section 2.2, we will encode the text to classify as a bag of words.\nThat is, we encode each review as a list of numbers, with each position in the list corresponding to a word in our vocabulary, and the value stored in that position corresponding to the number of times the word appears in the review.\nFor example, say we want to encode the following two reviews: \n\n4 https://ai.stanford.edu/~amaas/data/sentiment/ 5 https://www.imdb.com/ \n\n\n\nMaas et al. \n\n \n\n4.1 Binary Classification 57 \n\nTable 4.1 Two examples of movie reviews from IMDb.\nThe first is a positive review of the movie Puss in Boots (1988).\nThe second is a negative review of the movie Valentine (2001).\nThese reviews can be found at https://www.imdb.com/review/rw0606396/ and https://www.imdb.com/review/rw0721861/, respectively. \n\n  \n\nFilename Score Binary Label \n\ntrain/pos/24_8.txt 8/10 Positive \n\ntrain/neg/141_3.txt 3/10 Negative \n\nReview Text \n\nAlthough this was obviously a low-budget production, the per- formances and the songs in this movie are worth seeing.\nOne of Walken\u2019s few musical roles to date.\n(he is a mar- velous dancer and singer and he demonstrates his acrobatic skills as well - watch for the cartwheel!)\nAlso starring Ja- son Connery.\nA great children\u2019s story and very likable charac- ters. \n\nThis stalk and slash turkey manages to bring nothing new to an increasingly stale genre.\nA masked killer stalks young, pert girls and slaughters them in a variety of gruesome ways, none of which are particularly inventive.\nIt\u2019s not scary, it\u2019s not clever, and it\u2019s not funny.\nSo what was the point of it? \n\n   \n\nReview 1: Review 2: \n\n\"I liked the movie.\nMy friend liked it too.\n\"\n\n\"I hated it.\nWould not recommend.\n\"\n\nFirst, we need to create a vocabulary that maps each word to an id that uniquely identifies it.\nEach of these numbers will be used as the index in a list, so they must start at zero and grow by one for each word in the vocabulary.\nFor example, one possible vocabulary that encodes the previous reviews is: \n\n    {'would': 0,\n\n     'hated': 1,\n\n58 \n\nImplementing Text Classification Using Perceptron and LR \n\n   'my': 2,\n\n   'liked': 3,\n\n   'not': 4,\n\n   'it': 5,\n\n   'movie': 6,\n\n   'recommend': 7,\n\n   'the': 8,\n\n   'I': 9,\n\n   'too': 10,\n\n   'friend': 11}\n\nUsing this mapping, we can encode the two reviews as follows:\nReview1:\n[0,0,1,2,0,1,1,0,1,1,1,1] \n\nReview2:\n[1,1,0,0,1,1,0,1,0,1,0,0] \n\nNote that the word liked (fourth position) in the first review has a value of two.\nThis is because this word appears twice in that review. \n\nThis is a small example with a vocabulary of only 12 terms.\nOf course, the same process needs to be implemented for our whole training dataset.\nFor this purpose we will use scikit-learn\u2019s CountVectorizer\nclass.6 Using the CountVectorizer class simplifies things, allowing us to get started quickly with a bag-of-words approach.\nHowever, note that it makes sev- eral simplifying assumptions (e.g., text is lowercased, and punctuation and single character tokens are removed).\nSome of these may not be adequate to other tasks. \n\nFirst, we need to obtain the filenames for the reviews in the training set: \n\nOnce we have acquired the filenames for the training reviews, we need\u2028to read them using the CountVectorizer.\nIn order for the CountVectorizer to open and read the files for us, we make use of the input='filename' constructor parameter (otherwise it would expect the string content di- rectly).\nThe CountVectorizer provides three methods that will be use-\u2028ful for us: a method called fit() that is used to acquire the vocabulary,\u2028a method transform() that converts the text into the bag-of-words rep- resentation, and a method fit_transform() that conveniently acquires the vocabulary and transforms the data in a single step.\nThe resulting object is referred to as a document-term matrix, where each row corre- \n\n6 https://scikit- learn.org/stable/modules/generated/sklearn.feature_ extraction.text.CountVectorizer.html \n\n \n\n4.1 Binary Classification 59 \n\nsponds to a document, and each column corresponds to a term in the vocabulary. \n\nAs the output above indicates, the resulting matrix has 25,000 rows (one for each review), and 74,849 columns (one for each term).\nAlso you may note that this matrix is sparse, with 3,445,861 stored elements.\nA regular matrix of shape 25,000\u00d774,849 would have 1,871,225,000 elements.\nHowever, most of the elements in a document-term matrix are zeros because only a few words from the vocabulary appear in each document.\nA sparse matrix takes advantage of this fact by storing only the non-zero cells in order to reduce the memory required to store it.\nThus, sparse matrices are convenient, especially when dealing with lots of data.\nNevertheless, to simplify the downstream code in this example, we will convert it into a dense matrix, i.e., a regular two-dimensional NumPy array. \n\nFinally, we also need the labels of the reviews.\nWe assign a label of one to positive reviews, and a label of zero to negative ones.\nNote that the first half of the reviews are positive and the second half are negative.\nThe label at the ith position of the y_train array corresponds to the review encoded in the ith row of the X_train matrix. \n\n4.1.3 Perceptron \n\nNow that we have defined our task and the data processing pipeline, we will implement a perceptron classifier that classifies the movie reviews as positive or negative.\nThe entire code discussed in this section is available in the chap4_perceptron notebook.\nRecall from Section 2.4 that the perceptron is composed of a weight vector w and a bias term b.\nThese will be represented as a NumPy array w of the same length as our document vectors, and a variable b for the bias term.\nBoth will be initialized with zeros. \n\nThe parameters w and b are learned through the following algorithm, which implements Algorithm 2 from Chapter 2: \n\nThere are a couple of details to point out.\nLine 3 of Algorithm 2 indicates that we need to repeat the training loop until convergence.\nTheoretically, convergence is defined as predicting all training examples correctly.\nThis is an ambitious requirement, which is not always possible in practice, so in this code we also include a stop condition if we reach a maximum number of epochs.\nAnother crucial difference between our im- plementation here and the theoretical Algorithm 2, is that we randomize the order in which the training examples are seen at the beginning of \n\n60 Implementing Text Classification Using Perceptron and LR \n\neach epoch.\nThis simple (but highly recommended!)\nchange is necessary to avoid the introduction of spurious biases due to the arbitrary order of the examples in the original training partition.7 We accomplish this by storing the indices corresponding to the X_train matrix rows in a NumPy array, and shuffling these indices at the beginning of each epoch.\nWe shuffle the indices instead of the examples so that we can preserve the mapping between examples and labels. \n\nThe training loop aligns closely with Algorithm 2.\nWe start by iter- ating over each example in our training data, storing the current ex- ample in the variable x,8 and its corresponding label in the variable y_true.\nNext, we compute the perceptron decision function shown in Algorithm 1.\nNote that NumPy (as well as PyTorch) uses Python\u2019s @ operator to indicate vector or matrix multiplication, depending on its operand types.\nHere we use it to calculate the dot product of the exam- ple x and the weights w.\nTo this we add the bias b to obtain the predicted score, whose sign is used to assign a positive or negative predicted label.\nIf the prediction is correct, then no update is needed, and we can move on to the next training example.\nHowever, if the prediction is incorrect, then we need to adjust w and b, as described in Algorithm 2. \n\nSidebar 4.1 The tqdm function \n\nThis is our first exposure to the tqdm function.\ntqdm is a progress bar that \u201cmake your loops show a smart progress meter.\u201d9\nThe name tqdm comes from the Arabic word taqaddum which can mean \u201cprogress.\u201d\nUsing tqdm is as simple as wrapping it around the collection to be traversed. \n\nAfter training, we evaluate the model\u2019s performance on the held- out test partition.\nThe test data is loaded similarly to the training partition, but with one notable difference; we use CountVectorizer\u2019s transform() method instead of the fit_transform() method so that the vocabulary is not adjusted for the test data.\nWe won\u2019t show here the loading of the test partition since it is so similar to the code already shown, but it is available in the Jupyter notebook that accompanies this section. \n\n\t.\t\n7 \u00a0\nAs an extreme example, consider a dataset where all the positive examples appear first in the training partition.\nThis would cause the perceptron to artificially inflate the weights of the features that occur in these examples, a situation from which the learning algorithm may struggle to recover. \u2028\n\n\t.\t\n8 \u00a0We use typewriter font when we discuss variables in the code, to distinguish code from the theoretical discussion in the other chapters. \u2028\n\n9 https://github.com/tqdm/tqdm \n\n   \n\n4.1 Binary Classification 61 \n\nUsing the model to assign labels to all the test data is easily done in one step \u2013 we simply multiply the entire test data document-term matrix by the previously learned weights and add the bias.\nScores greater than zero indicate a positive review, and those less than zero are negative. \n\nAt this point we can evaluate the classifier\u2019s performance, which we will do using precision, recall, and F1 scores for binary classification (de- scribed in Section 2.3).\nFor this purpose, we implement a function called binary_classification_report that computes these metrics and re- turns them as a dictionary: \n\nWe call this function to compare the predicted labels to the true labels, and obtain the evaluation scores. \n\nOur F1 score here is 86.8%, which is much higher than the baseline that assigns labels randomly, which yields an F1 score of about 50%.\nThis is a good result, especially considering the simplicity of the perceptron!\nIn the next sections and chapters, we will discuss a battery of strategies to considerably improve this performance. \n\n4.1.4 Binary Logistic Regression from Scratch \n\nUsing the same task, dataset, and evaluation, we will now implement a logistic regression classifier, as described in Algorithm 5 from Chapter 3.\nTo give the reader hands-on experience with the implementation of the gradient calculations for logistic regression, we start by implementing it from scratch using NumPy.\nAll the code shown in this section is available in the chap4_logistic_regression_numpy notebook. \n\nIn the perceptron implementation, we represented the weights and the bias as two different variables.\nHere, however, we will use a different approach that will allow us to unify them into a single vector variable.\nSpecifically, we take advantage of the similarity between the derivative of the cost function with respect to the weights (Equation 3.14) and the derivative of the cost with respect to the bias (Equation 3.15). \n\nd Ci(w, b) = (\u03c3i \u2212 yi)xij (3.14 revisited) dwj \n\nd Ci(w, b) = \u03c3i \u2212 yi (3.15 revisited) db \n\nNote that the two derivative formulas are identical except that the for- mer has a multiplication by xij, while the latter does not.\nHowever, \n\n62 Implementing Text Classification Using Perceptron and LR since \n\n\u03c3i \u2212 yi = (\u03c3i \u2212 yi)1 \n\nwe can multiply the derivative of the cost with respect to the bias by one without changing the semantics.\nThis gives an opportunity for combining the computations, doing them both in a single pass.\nThe idea is that we can treat the bias as a weight corresponding to a feature that always has a value of one. \n\nAs can be seen above, we created a NumPy array of ones of the same length as the number of examples in our training set (i.e., the number of rows in the data matrix).\nThen we add this array as a new column to the data matrix, using NumPy\u2019s column_stack function. \n\nNext, we need to initialize our model.\nThis time we will use a single NumPy array w of the same length as the number of columns in the data matrix.\nThe weight vector w is initialized randomly with values between 0 and 1: \n\nBefore implementing the learning algorithm, we need an implementa- tion of the logistic function.\nRecall that the logistic function is \n\n\u03c3(x) = 1 (3.1 revisited) 1+e\u2212x \n\nThis function can be easily implemented in NumPy as follows: \n\nHowever, this naive implementation may produce the following warn- ing during training: \n\nThe term overflow indicates that the result of evaluating exp(-x) is a number so large that it can\u2019t be represented by a float (specifically, we\u2019re using float64 numbers).\nWe will avoid this issue by not calling exp with values that will overflow.\nNumPy provides the function finfo that can be consulted to find the limits of floating point numbers: \n\nThe log of the largest floating point number is the largest number for which exp() will not overflow, so we will use it as a threshold to filter out problematic values: \n\nWe now have everything we need to implement Algorithm 4.\nThe steps to follow for each example are: (1) use the model to make a prediction, (2) calculate the gradient of the loss function with respect to the model parameters, and (3) update the model parameters using the gradient.\nThe size of the update is controlled by the learning rate. \n\nOnce the model has been trained, we evaluate it on the test dataset using our binary_classification_report function from the previous section.\nLoading and preprocessing the test dataset follows the same \n\n \n\n4.1 Binary Classification 63 \n\nsteps as with the previous classifier.\nWe omit the code for brevity.\nThese are the results: \n\nThe performance is comparable with that of the perceptron.\nThe dif- ference in F1 scores between the two classifiers (84.9% here vs. 86.8% for the perceptron) is not significant.\nClassifier parity is probably at- tributable to the fact that the signal distinguishing the two classes be- ing easy to learn and the simpler perceptron training algorithm being su\ufb00icient in this case.\nNevertheless, this task is useful in showing how to implement the logistic regression model from scratch, i.e., by implement- ing the gradient calculation and parameter updates manually.\nNext, we will implement the same model again using PyTorch, highlighting how this machine learning library simplifies the process. \n\n4.1.5 Binary Logistic Regression Utilizing PyTorch \n\nWhile it is fairly straightforward to compute the derivatives for logistic regression and implement then directly in NumPy, this will not scale well to arbitrary neural architectures.\nFortunately, there are libraries that automate the computation of the derivatives of the cost function (assuming it is differentiable!)\nfor any neural network, and use the re- sulting gradients to perform gradient descent or other more sophisti- cated optimization procedures.\nTo this end, we will use the PyTorch deep learning library10.\nThe corresponding notebook for this section is chap4_logistic_regression_pytorch_bce. \n\nOur model for logistic regression corresponds to PyTorch\u2019s Linear layer.\nWhen we instantiate this layer, we specify the size of the inputs (the size of our vocabulary) and the size of the output, i.e., the number of output neurons (which is one because we\u2019re doing binary classification).\nThe loss function we use is the binary cross-entropy loss (see Chapter 3), which is implemented as BCEWithLogitsLoss in PyTorch.\nIn PyTorch, the gradients obtained from the loss function are applied to the model by an optimizer object, which implements and applies an optimization algorithm.\nHere we will use the vanilla stochastic gradient descent opti- mizer; we set its learning rate to 0.1.\nThis is equivalent to the discussion in Section 3.2. \n\nSimilarly to the manual implementation, the steps required to train the model for a given training example are: (1) ensure the gradients are set to zeros, (2) apply the model to obtain a prediction, (3) calculate \n\n10 https://pytorch.org/ \n\n \n\n64 Implementing Text Classification Using Perceptron and LR \n\nthe loss, (4) compute the gradient of the loss by back-propagation, and (5) update the model parameters. \n\nRecall that in our previous implementation everything was hardcoded: applying the model, computing the gradients, and optimizing the model parameters.\nHere, however, the implementation of the logistic regres- sion is expressed at a higher level of abstraction.\nThis means that we are describing the logical steps without specifying a particular imple- mentation.\nInstead, implementation details are the responsability of the chosen model, loss function, and optimizer.\nThus, we could even choose a different model, loss function, and/or optimizer, and use the same train- ing steps with little or no modification.\nThis decoupling of the training logic from the implementation details is one of the main advantages of libraries such as PyTorch. \n\nAs shown in the code above, calling the model as a function, with the feature vectors as inputs, produces the predicted scores.\nOnce again, a positive score corresponds to a positive label.\nWhen we evaluate this implementation on the test dataset, we obtain results that are in line with our previous models: \n\nWriting the perceptron and the logistic regression from scratch is a good exercise, as it exposes us to the fundamentals of implementing machine learning algorithms.\nHowever, this becomes cumbersome for more complex neural architectures.\nFor this reason, from this point on, we will use PyTorch for all our coding examples. \n\n4.2 Multiclass Classification \n\nSo far, in this chapter we have discussed implementing binary classi- fiers.\nNext, we will modify these binary classifiers to perform multiclass classification, following the discussion in Section 3.5. \n\n4.2.1 AG News Dataset \n\nBefore explaining the actual training/testing code, we have to choose a new dataset that is suitable for multiclass classification.\nTo this end, we will use the AG News Classification Dataset (Zhang et al., 2015), a subset of the larger AG corpus of news articles collected from thousands of different news sources.11\nThe classification dataset consists of four \n\n11 http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html \n\n  \n\n4.2 Multiclass Classification 65 \n\nclasses, and the data is equally balanced across all classes (30,000 articles per class for train, and 1,900 articles per class for testing).\nThe goal of the task is to classify each article as one of the four classes: World, Sports, Business, or Sci/Tech. \n\n4.2.2 Preparing the Dataset \n\nThe AG News Dataset is distributed as two CSV files (one for training and one for testing), each containing three columns: the class index, the title, and the description.\nThe dataset also provides a text file that maps the above class indexes to more descriptive class labels. \n\nBecause of the tabular nature of the dataset, pandas, a Python library\u2028for tabular data analysis,12 is a natural choice for loading and transform-\u2028ing it.\nTo this end, our Jupyter notebook (chap4_multiclass_logistic_regression) demonstrates the sequence of steps required to handle the data, as well\u2028as model training and evaluation.\nFirst, we show how to load the CSV,\u2028add column names, and inspect the result: \n\nclass index \n\n\t.\t\n0 \u00a03 \u2028\n\n\t.\t\n1 \u00a03 \u2028\n\n\t.\t\n2 \u00a03 \u2028\n\n\t.\t\n3 \u00a03 \u2028\n\n\t.\t\n4 \u00a03 \u2028\n\n... ... \n\n\t.\t\n119995 \u00a01 \u2028\n\n\t.\t\n119996 \u00a02 \u2028\n\n\t.\t\n119997 \u00a02 \u2028\n\n\t.\t\n119998 \u00a02 \u2028\n\n\t.\t\n119999 \u00a02 \u2028\n\ntitle Wall St. Bears Claw Back Into the Black (Reuters) Carlyle Looks Toward Commercial Aerospace (Reu... Oil and Economy Cloud Stocks' Outlook (Reuters) Iraq Halts Oil Exports from Main Southern Pipe...\nOil prices soar to all-time record, posing new... ...\nPakistan's Musharraf Says Won't Quit as Army C...\nRenteria signing a top-shelf deal Saban not going to Dolphins yet\nToday's NFL games Nets get Carter from Raptors \n\ndescription Reuters - Short-sellers, Wall Street's dwindli...\nReuters - Private investment firm Carlyle Grou... Reuters - Soaring crude prices plus worries\\ab... Reuters - Authorities have halted oil export\\f... AFP - Tearaway world oil prices, toppling reco... ...\nKARACHI (Reuters) - Pakistani President Perve...\nRed Sox general manager Theo Epstein acknowled...\nThe Miami Dolphins will put their courtship of... PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...\nINDIANAPOLIS -- All-Star Vince Carter was trad... \n\n \n\n120000 rows \u00d7 3 columns \n\nSince the class labels themselves are in a separate file, we manually add them to the pandas data structure (called dataframe in pandas\u2019 terminology) to increase the interpretability of the data.\nWe use the class index column as a starting point, and use its map method to create a new column with the corresponding labels (technically a new Series object) that is added to the dataframe using its insert method, which allows us to insert the column in a specific position.\nNote that the label indices are one-based, so we subtract one to align them with their labels. \n\n12 https://pandas.pydata.org \n\n \n\n66 Implementing Text Classification Using Perceptron and LR \n\nclass index \n\n\t.\t\n0 \u00a03 \u2028\n\n\t.\t\n1 \u00a03 \u2028\n\n\t.\t\n2 \u00a03 \u2028\n\n\t.\t\n3 \u00a03 \u2028\n\n\t.\t\n4 \u00a03 \u2028\n\n... ... \n\n\t.\t\n119995 \u00a01 \u2028\n\n\t.\t\n119996 \u00a02 \u2028\n\n\t.\t\n119997 \u00a02 \u2028\n\n\t.\t\n119998 \u00a02 \u2028\n\n\t.\t\n119999 \u00a02 \u2028\n\nclass Business Business Business Business Business ...\nWorld Sports Sports Sports Sports \n\ntitle Wall St. Bears Claw Back Into the Black (Reuters) \n\nOil and Economy Cloud Stocks' Outlook (Reuters)\nOil prices soar to all-time record, posing new...\nPakistan's Musharraf Says Won't Quit as Army C...\nSaban not going to Dolphins yet Nets get Carter from Raptors \n\ndescription Reuters - Short-sellers, Wall Street's dwindli... \n\nReuters - Soaring crude prices plus worries\\ab... AFP - Tearaway world oil prices, toppling reco...\nKARACHI (Reuters) - Pakistani President Perve...\nThe Miami Dolphins will put their courtship of... INDIANAPOLIS -- All-Star Vince Carter was trad... \n\n \n\nIraq Halts Oil Exports from Main Southern Pipe... Reuters - Authorities have halted oil export\\f... \n\n... ... \n\nRenteria signing a top-shelf deal Red Sox general manager Theo Epstein acknowled... \n\n120000 rows \u00d7 4 columns \n\nCarlyle Looks Toward Commercial Aerospace (Reu... Reuters - Private investment firm Carlyle Grou... \n\nToday's NFL games PITTSBURGH at NY GIANTS Time: 1:30 p.m.\nLine: ... \n\nNext we will preprocess the text.\nFirst we lowercase the title and description, and then we concatenate them into a single string.\nThen we remove some spurious backslashes from the text.\nOnce this is done, the preprocessed text is added to the dataframe as a new column.\nNote that pandas allows these steps to be applied to all rows simultaneously. \n\nclass index \n\nclass \n\ntitle \n\nWall St. Bears Claw Back Into the Black (Reuters) \n\nOil and Economy Cloud Stocks' Outlook (Reuters) \n\nOil prices soar to all-time record, posing new... \n\n... \n\nPakistan's Musharraf Says Won't Quit as Army C... \n\nSaban not going to Dolphins yet \n\nNets get Carter from Raptors \n\ndescription Reuters - Short-sellers, Wall Street's dwindli... \n\nReuters - Soaring crude prices plus worries\\ab... \n\nAFP - Tearaway world oil prices, toppling reco... \n\n... \n\nKARACHI (Reuters) - Pakistani President Perve... \n\nThe Miami Dolphins will put their courtship of... \n\nINDIANAPOLIS -- All-Star Vince Carter was trad... \n\ntext \n\nwall st. bears claw back into the black (reute... \n\noil and economy cloud stocks' outlook (reuters... \n\noil prices soar to all-time record, posing new... \n\n... \n\npakistan's musharraf says won't quit as army c... \n\nsaban not going to dolphins yet the miami dolp... \n\nnets get carter from raptors indianapolis -- a... \n\n  \n\n\t.\t0 \u00a03 Business \u2028\n\n\t.\t1 \u00a03 Business \u2028\n\n\t.\t2 \u00a03 Business \u2028\n\n\t.\t3 \u00a03 Business \u2028\n\n\t.\t4 \u00a03 Business \u2028\n\n... ... ... \n\n\t.\t119995 \u00a01 World \u2028\n\n\t.\t119996 \u00a02 Sports \u2028\n\n\t.\t119997 \u00a02 Sports \u2028\n\n\t.\t119998 \u00a02 Sports \u2028\n\n\t.\t119999 \u00a02 Sports \u2028\n\n120000 rows \u00d7 5 columns \n\nCarlyle Looks Toward Commercial Reuters - Private investment firm Carlyle carlyle looks toward commercial Aerospace (Reu... Grou... aerospace (reu... \n\nIraq Halts Oil Exports from Main Southern Pipe... \n\nReuters - Authorities have halted oil export\\f... \n\niraq halts oil exports from main southern pipe... \n\n  \n\nRenteria signing a top-shelf deal \n\nRed Sox general manager Theo Epstein renteria signing a top-shelf deal red sox acknowled... gene... \n\nToday's NFL games \n\nPITTSBURGH at NY GIANTS Time: 1:30 p.m.\ntoday's nfl games pittsburgh at ny giants Line: ... time... \n\nAt this point, the text is ready to be tokenized.\nFor this purpose we will use NLTK\u2019s word_tokenize function.\nThis function can be applied to the whole column at once using the pandas map function, which returns a new column which we add to the dataframe.\nHowever, here we actually use the progress_map function, which provides a visual progress bar.\nThis visual feedback is especially helpful for tasks that take more time to complete. \n\n4.2 Multiclass Classification 67 \n\nclass index \n\n\t.\t\n0 \u00a03 \u2028\n\n\t.\t\n1 \u00a03 \u2028\n\n\t.\t\n2 \u00a03 \u2028\n\n\t.\t\n3 \u00a03 \u2028\n\n\t.\t\n4 \u00a03 \u2028\n\n... ... \n\n\t.\t\n119995 \u00a01 \u2028\n\n\t.\t\n119996 \u00a02 \u2028\n\n\t.\t\n119997 \u00a02 \u2028\n\n\t.\t\n119998 \u00a02 \u2028\n\n\t.\t\n119999 \u00a02 \u2028\n\nclass Business Business Business Business Business \n\n...\nWorld \n\nSports Sports Sports Sports \n\ntitle \n\nWall St. Bears Claw Back Into the Black (Reuters) \n\nOil and Economy Cloud Stocks' Outlook (Reuters) \n\nOil prices soar to all-time record, posing new... \n\n... \n\nPakistan's Musharraf Says Won't Quit as Army C... \n\nSaban not going to Dolphins yet \n\nNets get Carter from Raptors \n\ndescription \n\nReuters - Short-sellers, Wall Street's dwindli... \n\nReuters - Soaring crude prices plus worries\\ab... \n\nAFP - Tearaway world oil prices, toppling reco... \n\n... \n\nKARACHI (Reuters) - Pakistani President Perve... \n\nThe Miami Dolphins will put their courtship of... \n\nINDIANAPOLIS -- All-Star Vince Carter was trad... \n\ntext \n\nwall st. bears claw back into the black (reute... \n\noil and economy cloud stocks' outlook (reuters... \n\noil prices soar to all-time record, posing new... \n\n... \n\npakistan's musharraf says won't quit as army c... \n\nsaban not going to dolphins yet the miami dolp... \n\nnets get carter from raptors indianapolis -- a... \n\ntokens \n\n[wall, st., bears, claw, back, into, the, blac... \n\n[oil, and, economy, cloud, stocks, ', outlook,... \n\n[oil, prices, soar, to, all-time, record, ,, p... \n\n... \n\n[pakistan, 's, musharraf, says, wo, n't, quit,... \n\n[saban, not, going, to, dolphins, yet, the, mi... \n\n[nets, get, carter, from, raptors, indianapoli... \n\n       \n\n120000 rows \u00d7 6 columns \n\nCarlyle Looks Toward Commercial Reuters - Private investment firm carlyle looks toward commercial [carlyle, looks, toward, Aerospace (Reu... Carlyle Grou... aerospace (reu... commercial, aerospace... \n\nIraq Halts Oil Exports from Main Reuters - Authorities have halted iraq halts oil exports from main [iraq, halts, oil, exports, from, Southern Pipe... oil export\\f... southern pipe...\nmain, southe... \n\nRenteria signing a top-shelf deal \n\nRed Sox general manager Theo renteria signing a top-shelf deal [renteria, signing, a, top-shelf, Epstein acknowled... red sox gene...\ndeal, red, s... \n\nToday's NFL games \n\nPITTSBURGH at NY GIANTS today's nfl games pittsburgh at [today, 's, nfl, games, Time: 1:30 p.m.\nLine: ...\nny giants time...\npittsburgh, at, ny, gi... \n\nFrom the tokens we just created, we then create a vocabulary for our corpus.\nHere, we only keep the words that occur at least 10 times, decreasing the memory needed and reducing the likelihood that our vo- cabulary contains noisy tokens.\nNote that each row in the tokens column contains a list of tokens.\nIn order to create the vocabulary, we will need to convert the Series of lists of tokens into a Series of tokens using the explode() Pandas method.\nThen we will use the value_counts() method to create a Series object in which the index are the tokens and the values are the number of times they appear in the corpus.\nThe next step is removing the tokens with a count lower than our chosen threshold.\nFinally, we create a list with the remaining tokens, as well as a dictionary that maps tokens to token ids (i.e., the index of the token in the list).\nWe include in the vocabulary a special token\n[UNK] that will be used as a placeholder for tokens that do not appear in our vocabulary after the frequency pruning. \n\nUsing this vocabulary, we construct a feature vector for each news article in the corpus.\nThis feature vector will be encoded as a dictionary, with keys corresponding to token ids, and values corresponding to the number of times the token appears in the article.\nAs above, the feature vectors will be stored as a new column in the dataframe. \n\n68 Implementing Text Classification Using Perceptron and LR \n\nclass index \n\nclass \n\ntitle \n\nWall St. Bears Claw Back Into the Black (Reuters) \n\nOil and Economy Cloud Stocks' Outlook (Reuters) \n\nOil prices soar to all-time record, posing new... \n\n... \n\nPakistan's Musharraf Says Won't Quit as Army C... \n\nSaban not going to Dolphins yet \n\nNets get Carter from Raptors \n\ndescription \n\nReuters - Short-sellers, Wall Street's dwindli... \n\nReuters - Soaring crude prices plus worries\\ab... \n\nAFP - Tearaway world oil prices, toppling reco... \n\n... \n\nKARACHI (Reuters) - Pakistani President Perve... \n\nThe Miami Dolphins will put their courtship of... \n\nINDIANAPOLIS -- All-Star Vince Carter was trad... \n\ntext \n\nwall st. bears claw back into the black (reute... \n\noil and economy cloud stocks' outlook (reuters... \n\noil prices soar to all-time record, posing new... \n\n... \n\npakistan's musharraf says won't quit as army c... \n\nsaban not going to dolphins yet the miami dolp... \n\nnets get carter from raptors indianapolis -- a... \n\ntokens \n\n[wall, st., bears, claw, back, into, the, blac... \n\n[oil, and, economy, cloud, stocks, ', outlook,... \n\n[oil, prices, soar, to, all- time, record, ,, p... \n\n... \n\n[pakistan, 's, musharraf, says, wo, n't, quit,... \n\n[saban, not, going, to, dolphins, yet, the, mi... \n\n[nets, get, carter, from, raptors, indianapoli... \n\nfeatures \n\n{427: 2, 563: 1, 1607: 1, 15062: 1, 120: 1, 73... \n\n{66: 1, 9: 2, 351: 2, 4565: 1, 158: 1, 116: 1,... \n\n{66: 2, 99: 2, 4390: 1, 4: 2, 3595: 1, 149: 1,... \n\n... \n\n{383: 1, 23: 1, 1626: 2, 91: 1, 1809: 1, 285: ... \n\n{7762: 2, 68: 1, 661: 1, 4: 2, 1439: 2, 703: 1... \n\n{2170: 2, 226: 1, 2402: 2, 32: 1, 2995: 2, 219... \n\n   \n\n\t.\t0 \u00a03 Business \u2028\n\n\t.\t1 \u00a03 Business \u2028\n\n\t.\t2 \u00a03 Business \u2028\n\n\t.\t3 \u00a03 Business \u2028\n\n\t.\t4 \u00a03 Business \u2028\n\n... ... ... \n\n\t.\t119995 \u00a01 World \u2028\n\n\t.\t119996 \u00a02 Sports \u2028\n\n\t.\t119997 \u00a02 Sports \u2028\n\n\t.\t119998 \u00a02 Sports \u2028\n\n\t.\t119999 \u00a02 Sports \u2028\n\n120000 rows \u00d7 7 columns \n\n\n\nCarlyle Looks Toward Commercial Aerospace (Reu... \n\nReuters - Private investment firm Carlyle Grou... \n\ncarlyle looks toward commercial aerospace (reu... \n\n\n\nIraq Halts Oil Exports from Reuters - Authorities have iraq halts oil exports from Main Southern Pipe... halted oil export\\f... main southern pipe... \n\n  \n\nRenteria signing a top-shelf Red Sox general manager renteria signing a top- deal Theo Epstein acknowled... shelf deal red sox gene... \n\nPITTSBURGH at NY Today's NFL games GIANTS Time: 1:30 p.m. Line: ... \n\ntoday's nfl games pittsburgh at ny giants time... \n\n[carlyle, looks, toward, {15999: 2, 1076: 1, 855: commercial, aerospace... 1, 1286: 1, 4251: 1, ... \n\n[iraq, halts, oil, exports, {77: 2, 7380: 1, 66: 3, from, main, southe... 1787: 1, 32: 2, 900: 2... \n\n \n\n[renteria, signing, a, top- {8428: 2, 2638: 1, 5: 4, shelf, deal, red, s... 0: 3, 127: 1, 202: 3,... \n\n\n\n[today, 's, nfl, games, {106: 1, 23: 1, 729: 1, pittsburgh, at, ny, gi... 225: 1, 1586: 1, 22: 1... \n\nThe final preprocessing step is converting the features and the class indices into PyTorch tensors.\nRecall that we need to subtract one from the class indices to make them zero-based. \n\nAt this point, the data is fully processed and we are ready to begin training. \n\n4.2.3 Multiclass Logistic Regression Using PyTorch \n\nThe model itself is a single linear layer whose input size corresponds to the size of our vocabulary, and its output size corresponds to the number of classes in our corpus.\nPyTorch\u2019s Linear layer includes a bias by default, so there is no need to handle that manually the way we did for our perceptron example. \n\nThe code for training this model (which implements Algorithm 6) is almost identical to that of the binary logistic repression.\nHowever, since we have to calculate a score for each of the four different classes, we need to replace the previous BCEWithLogitsLoss with CrossEntropyLoss, which applies a softmax over the scores to obtain probabilities for each class. \n\nFor each example, the model predicts 4 scores \u2013 one for each label.\nThe label with the highest score is selected using the argmax function.\nWe evaluate the predictions of our model for each class using Scikit- learn\u2019s classification_report, which handles the results of multiclass classification. \n\n4.3 Summary 69 4.3 Summary \n\nIn this chapter, we used movie review and news article classification to illustrate the implementation of the previously described algorithms for the binary perceptron, binary logistic regression, and multiclass logistic regression.\nFor the binary logistic regression, we made a direct compar- ison between the lower-level NumPy implementation and a higher-level version that made use of PyTorch. \n\nWe hope that through this series of exercises the reader has noted several key takeaways.\nFirst, data preparation is important and should be done thoughtfully.\nCertain tasks (e.g., text normalization or sentence splitting) are going to be frequently needed if you continue with NLP, so using or creating generic functions can be very helpful.\nHowever, what works for one dataset and one language may not be suitable for another scenario.\nFor example, in our case, we selected different tokenizers for each of our tasks to account for the different registers of English, as well as removing diacritics during normalization. \n\nSecond, when it comes to implementing machine learning algorithms, it is often easier to use a higher-level library such as PyTorch instead of NumPy.\nFor example, with the former, the gradients are calculated by the library, whereas in NumPy we have to code them ourselves.\nThis becomes cumbersome quickly.\nFor example, even the derivative of the softmax is non-trivial. \n\nThird, PyTorch imposes a training structure that remains largely the same, regardless of what models are being trained.\nThat is, at a high level, the same steps are always required: clearing the current gradients, predicting output scores for the provided inputs, calculating the loss, and optimizing.\nThese features make PyTorch a very powerful and convenient deep learning library; we will continue to use it throughout the remainder of the book to implement more complex neural architectures. \n",
		"repo_url": "https://github.com/clulab/gentlenlp/tree/e3e5a69ebff7e9bda7476d57631f708c0769527a"
	}
]