[{"id":15,"annotations":[{"id":12,"completed_by":1,"result":[{"value":{"start":235,"end":429,"text":"Remaining fairly simple, our network will consist of three neuron layers that are fully connected: an input layer that stores the input features, a hidden intermediate layer, and an output layer","labels":["Textual Description"]},"id":"7P0egzn7PL","from_name":"text_segment","to_name":"paper_text","type":"labels","origin":"manual"},{"value":{"start":235,"end":429,"text":["https:\/\/github.com\/clulab\/gentlenlp\/blob\/e3e5a69ebff7e9bda7476d57631f708c0769527a\/python\/chap07_ffnn.py#L46-L48"]},"id":"7P0egzn7PL","from_name":"snippet_github_url","to_name":"paper_text","type":"textarea","origin":"manual"}],"was_cancelled":false,"ground_truth":false,"created_at":"2023-06-16T19:53:10.947942Z","updated_at":"2023-06-16T19:59:23.098375Z","lead_time":163.05700000000002,"prediction":{},"result_count":0,"task":15,"project":3,"parent_prediction":null,"parent_annotation":null}],"file_upload":"25e253be-gentle_nlp_data.json","drafts":[],"predictions":[],"data":{"paper_text":"7 \n\nImplementing Text Classification with Feed Forward Networks \n\nIn this chapter we provide an implementation of the multilayer neural network described in Chapter 5, along with several of the best practices discussed in Chapter 6. \n\nRemaining fairly simple, our network will consist of three neuron layers that are fully connected: an input layer that stores the input features, a hidden intermediate layer, and an output layer that produces the scores for each class to be learned.\nIn between these layers we will include dropout and a nonlinearity (ReLU). \n\nSidebar 7.1 The PyTorch Linear layer implements the connections between layers of neurons. \n\nBefore discussing the implementation of more complex neural archi- tectures in PyTorch, it is important to address one potential source of confusion.\nIn PyTorch, the Linear layer implements the connections be- tween two layers of neurons rather than an actual neuron layer.\nThat is, a Linear object contains the weights Wl+1 that connect the neu- rons in layer l with the neurons in layer l + 1 in Figure 5.2.\nThis is why the Linear constructor includes two dimensions: one for the in- put neuron layer (in_features) and one for the output neuron layer (out_features).\nOptionally, if the parameter bias is set to True, the corresponding Linear object also contains the bias weights for the out- put neurons, i.e., bl+1 in Figure 5.2.\nThus, in our Model with three neuron layers, we will have two Linear objects. \n\nTo stay close to the code, from this point forward when we mention the term layer in the implementation chapters, we refer to a PyTorch Linear layer, unless stated otherwise. \n\n109 \n\n  \n\n110 Implementing Text Classification with Feed Forward Networks \n\nFurther, we make use of two PyTorch classes: a Dataset and a DataLoader.\nThe advantage of using these classes is that they make several things easy, including data shuffling and batching.\nLastly, since the classifier’s architecture has become more complex, for optimization we transition from stochastic gradient descent to the Adam optimizer to take advan- tage of its additional features such as momentum, and L2 regularization. \n\nAs before, the code from this chapter is available in a Jupyter notebook: chap7_ffnn. \n\n7.1 Data \n\nIn this chapter we continue to use the AG News Dataset (Section 4.2.1), including the same loading and preprocessing steps.\nAlso, we continue using the same train and test sets to be able to compare results to the ones obtained in Section 4.2.\nHowever, in this chapter we will make use of a development set to tune the model’s hyper parameters.\nFor this purpose, we split the training set in two: 80% of the examples become a new training set, while the other 20% are the development set: \n\nIn the code above we used scikit-learn’s train_test_split function to split the training set into a development partition and a new training partition.\nNote that this function can split Python lists, NumPy arrays, and even Pandas dataframes.\nThe returned dataframes preserve the in- dex of the original training dataframe, which can be useful to keep the connection to the original data, but is not what we currently need, as we are trying to create two independent datasets.\nTherefore, we reset the index of the two new dataframes. \n\nA second difference to what was done in Section 4.2 is the introduction of mini-batches.\nPyTorch provides the DataLoader1 class which can be used for shuffling the data and splitting it into mini-batches.\nIn order to create a DataLoader, we need the data to be in the form of a PyTorch Dataset.2\nThere are two main types of PyTorch datasets: map-style and iterable-style.\nWe will use the former, as it is simpler and meets our needs, but it is good to know that the other option is available for situations when, for example, you need to stream data from a remote source or random access is expensive. \n\nTo create a map-style dataset we need to subclass torch.utils.data.\nDataset and override its __getitem__() method (to return an example given a \n\n1 https:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.DataLoader 2 https:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.Dataset \n\n  \n\n7.2 Fully-Connected Neural Network 111 \n\nkey), as well as its __len__() method (to return the number of exam- ples in the dataset).\nOur dataset implementation stores two sequences: one for holding the features, and another for storing the corresponding labels.\nIn our implementation we store two Pandas Series, but Python lists or NumPy arrays would also work.\nThe implementation __len__() is trivial: we simply return the length of the feature sequence, that is, the number of feature vectors.\nThe implementation of __getitem__() is slightly more involved.\nRecall that each of our feature vectors is rep- resented as a dictionary with word ids as keys, and word counts as values, and any word id not in the dictionary has a count of zero.\nOur __getitem__() method transforms this representation into one that Py- Torch can use.\nWe first create two PyTorch tensors, one for the label and one for the features, which is initially populated with zeros.\nThen, we retrieve the feature dictionary corresponding to the provided index, and, for each key-value pair in the feature dictionary, we update the corre- sponding element of the tensor.\nOnce this is complete, we return the feature and label tensors for the datum: \n\n7.2 Fully-Connected Neural Network \n\nHaving completed the Dataset implementation, we next implement the model, i.e., a fully-connected neural network with two layers.3\nIn Sec- tion 4.2 we used a Linear module directly to implement the simpler models discussed there.\nThis time, we will demonstrate how to imple- ment a model as a new module, by subclassing torch.nn.\nModule.\nAl- though this is not necessary for this model, as it can be represented by a Sequential module, as models get more complex, it becomes help- ful to encapsulate their behavior.\nTo implement a Module, we need to implement the constructor and override the forward() method. \n\nNote that, in our constructor below, before initializing the object fields, we invoke the constructor of the parent class (i.e., Module) with the line super().__init__().\nThis allows PyTorch to set up the mech- anisms through which any layers defined as attributes in the construc- tor are properly registered as model parameters.\nIn our example, a Sequential instance is assigned to self.layers; this is enough for our model instance to know about it during back-propagation and parameter updating. \n\n3 Recall that layer here refers to the PyTorch Linear layer that contains the connections between two neuron layers.\nSee Sidebar 7.1 for more details. \n\n112 Implementing Text Classification with Feed Forward Networks \n\nHere, our model consists of two linear layers, each one preceded by a dropout layer (which drops out input neurons from the corresponding linear layer).\nThe input of the first linear layer has the same size as our vocabulary, and its output has the dimension of the hidden neuron layer (please see Section 5.1 for a refresher on the architecture of the feed-forward neural network).\nConsequently, the input size of the second linear layer is equal to the size of the hidden layer, and its output size is the number of classes.\nAdditionally, between the two linear layers we add a ReLU nonlinearity.4 All of the model layers are wrapped in a Sequential module, which simply connects the output of one layer to the input of the next. \n\nThe second method we need to implement is the forward() method, which defines how the model applies its layers to a given input during the forward pass.\nOur forward() method simply calls the sequential layer and returns its output.\nNote that while this method implements the model’s forward pass, in general, this method should not be called directly by the user.\nInstead, the user should use the model as though it were a function (technically, invoking the __call__() method), and let PyTorch call the forward() method internally.\nThis allows PyTorch to activate necessary features such as module hooks correctly. \n\n7.3 Training \n\nIn order to train our model, we will first initialize the hyperparameters and the different components we need: model, loss function, optimizer, dataset, and data-loader.\nNotable differences with respect to Section 4.2 are the use of the Adam optimizer with a weight decay (this is just what PyTorch calls L2 regularization – see Chapter 6), and the use of a data- loader with shuffling and batches of 500 examples.\nWe encourage you to take the time to examine the values we use for the hyper parameters, and to experiment with modifying them in the Jupyter notebook. \n\nThe basic steps of the learning loop are the same as those in Sec- \n\n4 Note that nonlinearities such as the ReLU function here are necessary to guarantee that the neural network can learn non-linear decision boundaries.\nSee Chapter 5 for an extended discussion on this topic.\nFurther, nonlinearities can be added after each network layer, but, typically, the output layer is omitted.\nThis is because a softmax or sigmoid function usually follows it.\nIn PyTorch, the nn.CrossEntropyFunction, which we also use in this chapter, includes such a softmax function. \n\n7.3 Training 113 tion 4.2, except that we are now using a development set to keep track \n\nof the performance of the current model after each training epoch. \n\nOne important difference between using our model during training and evaluation is that, prior to each training session, we need to set the model to training mode using the train() method, and before evalu- ating on the development set, we need to set the model to evaluation mode using the eval() method.\nThis is important, because some layers have different behavior depending on whether the model is in training or evaluation mode.\nIn our model, this is the case for the Dropout layer, which randomly zeroes some of its input elements during training and scales its outputs accordingly (see Section 6.6), but during evaluation does nothing. \n\nIn order to plot some relevant statistics acquired from the training data, we collect the current loss and accuracy for each mini-batch.\nNote that we call detach() on the tensors corresponding to the loss and the predicted\/gold labels so they are no longer considered when computing gradients.\nCalling cpu() copies the tensors from the GPU to the CPU if we are using the GPU; otherwise it does nothing.\nCalling numpy() converts the PyTorch tensor into a NumPy array.\nUnlike the prediction sequence, which is represented as a vector of label scores, the loss is a scalar.\nFor this reason, we retrieve it as a Python number using the item() method. \n\nWhen evaluating on the development set, since we do not need to compute the gradients, we save computation by wrapping the steps in a torch.no_grad() context-manager.\nSince we are not learning, we do not perform back-propagation or invoke the optimizer. \n\nAfter completing training we have gathered the loss and accuracy val- ues after each epoch for both the training and development partitions.\nNext, we plot these values in order to visualize the classifier’s progress over time.\nPlots such as these are important to determine how well our model is learning, which informs decisions regarding adjusting hyper pa- rameters or modifying the model’s architecture.\nBelow we only show the plot for the loss.\nPlotting the accuracy is very similar; the corresponding code as well as the plot itself is available in the Jupyter notebook. \n\n114 Implementing Text Classification with Feed Forward Networks \n\n                       \n\nThe plot indicates that both the training and development losses de- crease over time.\nThis is good!\nIt indicates that our classifier is neither overfitting nor underfitting.\nRecall from Chapter 2 that overfitting hap- pens when a classifier performs well in training, but poorly on unseen data.\nIn the plot above this would be indicated by a training loss that continues to decrease, but is associated with a development loss that does not.\nUnderfitting happens when a classifier is unable to learn meaningful associations between the input features and the output labels.\nIn this plot this would be shown as loss curves that do not decrease over time. \n\nThis analysis means we are ready to evaluate our trained model on the test set, which must be a truly unseen dataset that was not used for training or to tune hyper parameters.\nIn other words, this experiment will indicate how well our model performs “in the wild.”\nBecause we would like these results to be as close as possible to real-world results, the test set should be used sparingly, only after the entire architecture, its trained parameters, and its hyper parameters have been frozen. \n\nWith our feed-forward neural architecture we have achieved an accu- racy of 92%, which is a substantial improvement over the 88% accuracy we obtained in Section 4.2.\nWe strongly suggest that you experiment not only with the different hyper parameters, but also with different model architectures in the Jupyter notebook.\nSuch exercises will help you de- \n\n7.4 Summary 115 velop an intuition about the different effects each design choice has, as \n\nwell as how these decisions interact with each other. \n\n7.4 Summary \n\nIn this chapter we have shown how to implement a feed-forward neural network in PyTorch.\nWe have also introduced several PyTorch features that encourage and simplify deep learning best practices.\nIn particu- lar, the built-in Dataset and DataLoader classes make mini-batching straightforward while still allowing for customization such as sampling.\nThe ability to create a custom Dataset object allows us to handle com- plex data and still have access to the features of a DataLoader.\nBy convention, all the components provided by PyTorch are batch-aware and assume that the first dimension refers to the batch size, simplifying model implementation and improving readability. \n\nIn building the model itself, we also saw that PyTorch uses layer mod- ularization, i.e., both the network layers themselves and operations on them (such as dropout and activation functions) are modeled as layers in a pipeline.\nThis makes it easy to interweave network layers, add various operations between them, and swap activation functions as desired.\nThe weight initialization is also handled automatically when the layers are created, but can be customized as needed. \n\nFurther, one can tailor the training process in PyTorch by adding mo- mentum, adaptive learning rates, and regularization through optimizer selection and configuration.\nIn this chapter, we used the Adam opti- mizer, which, in the authors’ experience, is a good default choice, but there are many other optimizers to choose from.\nWe recommend that the reader read the PyTorch documentation on optimizers for more details: https:\/\/pytorch.org\/docs\/stable\/optim.html. \n\n \n","repo_url":"https:\/\/github.com\/clulab\/gentlenlp\/tree\/e3e5a69ebff7e9bda7476d57631f708c0769527a"},"meta":{},"created_at":"2023-06-16T19:39:32.947297Z","updated_at":"2023-06-16T19:59:23.161123Z","inner_id":5,"total_annotations":1,"cancelled_annotations":0,"total_predictions":0,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":3,"updated_by":1,"comment_authors":[]}]